稀疏自编码器-ReLU-SGD
===

#### This project differs from previous commits that Autoencoders adopts ReLU (Rectified Linear Units) and is trained by various versions of SGD. Previous counterpart AE is trained by a batch gradient based method, L-BFGS.

#### 这个项目中的自编码器和之前的项目不同，主要是它使用了ReLU节点，即使用了ReL作为神经网络的激活函数(之前的网络使用的是sigmoid函数)。同时编码器是用随机梯度下降法(SGD)和它的各种不同版本的改进训练而成，之前的神经网络是用L-BFGS训练而成的。
